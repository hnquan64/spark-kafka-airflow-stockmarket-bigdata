{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "df = spark.read.options(header='True',inferSchema='True',delimiter=',').csv('./BigData/Dataset/finalData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType \n",
    "\n",
    "# Stage 1: get ClassWeightCol\n",
    "# Create Class Weight Column Transformer\n",
    "class getClassWeightCol(Transformer):\n",
    "    def __init__(self, label):\n",
    "        super(getClassWeightCol, self).__init__()\n",
    "        self.label = label\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        up = df.filter(col(label) == 'up').count()\n",
    "        down = df.filter(col(label) == 'down').count()\n",
    "        sw = df.filter(col(label) == 'sw').count()\n",
    "        total = df.count()\n",
    "\n",
    "        wup = total/(3*up)\n",
    "        wdown = total/(3*down)\n",
    "        sw = total/(3*sw)\n",
    "\n",
    "        calculateWeights = udf(lambda x: wup if x == \"up\" else (wdown if x==\"down\" else (sw)),DoubleType())\n",
    "        return df.withColumn(\"classWeightCol\", calculateWeights(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import  DecisionTreeClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler, StandardScaler\n",
    "\n",
    "# Choose optine label\n",
    "label = 'pred_5_5p'\n",
    "\n",
    "\n",
    "#Stage 2: pre Features\n",
    "vec = VectorAssembler(inputCols=['close','14_period_RSI', '14_period_STOCH_K', 'MFV', '14_period_ATR', 'MOM', \\\n",
    "                                 '14_period_MFI', 'ROC', 'OBV', '20_period_CCI', '14_period_EMV', \\\n",
    "                                 'Williams', '14_period_ADX', '20_period_TRIX'], outputCol=\"NumFeatures\")\n",
    "\n",
    "# Stage 3: standardscaler Features\n",
    "standardscaler = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "# Stage 4: get Label\n",
    "labelIndexer = StringIndexer(inputCol=label, outputCol=\"indexLabel\")\n",
    "\n",
    "# Stage 5: get Features\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexFeatures\", maxCategories=4)\n",
    "\n",
    "# Stage 6: Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexLabel\", featuresCol=\"indexFeatures\", weightCol='classWeightCol')\n",
    "\n",
    "# Final pipeline\n",
    "pipeline_dt = Pipeline(stages=[getClassWeightCol(label), vec, standardscaler, labelIndexer, featureIndexer, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "# Split the data into training and test sets \n",
    "(trainingData, testData) = df.randomSplit(weights=[0.8,0.2], seed = 2000)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_dt = pipeline_dt.fit(trainingData)\n",
    "\n",
    "# model_lr = pipeline_lr.fit(trainingData)\n",
    "\n",
    "# model_rf = pipeline_rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model\n",
    "# modelname = 'DT'\n",
    "# model_dt.write().overwrite().save(\"./BigData/Model/\" + modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|prediction|indexLabel|            features|\n",
      "+----------+----------+--------------------+\n",
      "|       0.0|       0.0|[-0.3740466994898...|\n",
      "|       0.0|       0.0|[-0.3740595638688...|\n",
      "|       0.0|       0.0|[-0.3741682004229...|\n",
      "|       0.0|       0.0|[-0.3739600940591...|\n",
      "|       0.0|       0.0|[-0.3741051149762...|\n",
      "+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions_dt = model_dt.transform(testData)\n",
    "\n",
    "# predictions_lr = model_lr.transform(testData)\n",
    "\n",
    "# predictions_rf = model_rf.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions_dt.select(\"prediction\", \"indexLabel\", \"features\").show(5)\n",
    "\n",
    "# predictions_lr.select(\"prediction\", \"indexLabel\", \"features\").show(5)\n",
    "\n",
    "# predictions_rf.select(\"prediction\", \"indexLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy dt : 0.7163157694357725\n"
     ]
    }
   ],
   "source": [
    "# Test result\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_dt = evaluator.evaluate(predictions_dt)\n",
    "\n",
    "# accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "\n",
    "# accuracy_rf = evaluator.evaluate(predictions_rf)\n",
    "\n",
    "# print('accuracy dt lr rf:', accuracy_dt, accuracy_lr, accuracy_rf)\n",
    "print('accuracy dt :', accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.81      0.84    261560\n",
      "         1.0       0.18      0.27      0.22     29842\n",
      "         2.0       0.18      0.23      0.20     23790\n",
      "\n",
      "    accuracy                           0.72    315192\n",
      "   macro avg       0.41      0.44      0.42    315192\n",
      "weighted avg       0.76      0.72      0.74    315192\n",
      "\n",
      "[[212334  29559  19667]\n",
      " [ 16390   7983   5469]\n",
      " [ 12654   5676   5460]]\n"
     ]
    }
   ],
   "source": [
    "#Visualize result\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "y_true = predictions_dt.select(['indexLabel']).collect()\n",
    "y_pred = predictions_dt.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# #Visualize result\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# y_true = predictions_lr.select(['indexLabel']).collect()\n",
    "# y_pred = predictions_lr.select(['prediction']).collect()\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# #Visualize result\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# y_true = predictions_rf.select(['indexLabel']).collect()\n",
    "# y_pred = predictions_rf.select(['prediction']).collect()\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# print(confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
